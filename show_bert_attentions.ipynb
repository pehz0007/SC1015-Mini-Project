{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Visualisation of the BERT Attention using `bertviz` \n",
    "\n",
    "- Please run the `bert_model.ipynb` before running this nodebook."
   ],
   "id": "539dd9c827ee9800"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the Tokenizer model",
   "id": "bda34f68fb0a5be5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer_model_path = './models/BERT/'\n",
    "bert_model_path = './models/BERT/'\n",
    "\n",
    "# Load the tokenizer\n",
    "if os.path.exists(tokenizer_model_path):\n",
    "    print(\"Loading the existing tokenizer...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(tokenizer_model_path)\n",
    "else:\n",
    "    print(\"No existing tokenizer found... Please run the `bert_model.ipynb` notebook first.\")\n"
   ],
   "id": "8b82bfef6b7ae668"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the BERT model",
   "id": "ab36d4e9284544fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "config = BertConfig.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "# Load the BERT model\n",
    "if os.path.exists(bert_model_path):\n",
    "    print(\"Loading the existing model...\")\n",
    "    model_bert = TFBertForSequenceClassification.from_pretrained(bert_model_path, config=config)\n",
    "    model_bert.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "else:\n",
    "    print(\"No existing model found... Please run the `bert_model.ipynb` notebook first.\")\n",
    "    "
   ],
   "id": "17664ec77c58ead3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualise the attention score for the sentence `love it, a great upgrade from the original.  I've had mine for a couple of years`",
   "id": "cc175f01b5fdff08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from bertviz import head_view\n",
    "import torch\n",
    "\n",
    "sentence = 'love it, a great upgrade from the original.  I\\'ve had mine for a couple of years'\n",
    "encoded_dict = tokenizer.encode_plus(\n",
    "    sentence,\n",
    "    add_special_tokens = True,\n",
    "    max_length = 50,\n",
    "    pad_to_max_length = True,\n",
    "    return_attention_mask = True,\n",
    "    return_tensors = 'tf',\n",
    ")\n",
    "\n",
    "outputs = model_bert(input_ids=encoded_dict['input_ids'], attention_mask=encoded_dict['attention_mask'], return_dict=True)\n",
    "\n",
    "attention = outputs[-1]\n",
    "attention = [torch.from_numpy(layer_attn.numpy()) for layer_attn in attention]\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_dict['input_ids'][0])  "
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "attention",
   "id": "a3c905873eff652b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "head_view(attention, tokens)",
   "id": "4a69843b9ac04024"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
